
R version 2.11.1 (2010-05-31)
Copyright (C) 2010 The R Foundation for Statistical Computing
ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> ###
> # check glmboostLSS()
> 
> require("gamboostLSS")
Loading required package: gamboostLSS
Loading required package: mboost
Loading required package: survival
Loading required package: splines
> require("gamlss")
Loading required package: gamlss
Loading required package: gamlss.dist
Loading required package: gamlss.data
 **********   GAMLSS Version 4.0-0 ********** 
For more on GAMLSS look at http://www.gamlss.com/ 
Type gamlssNews() to see new features/changes/bug fixes.

Attaching package: 'gamlss'

The following object(s) are masked from 'package:survival':

    ridge

> 
> set.seed(1907)
> n <- 5000
> x1  <- runif(n)
> x2 <- runif(n)
> mu <- 2 -1*x1 - 3*x2
> sigma <- exp(-1*x1 + 3*x2)
> df <- exp(1 + 3*x1 + 1*x2)
> y <- rTF(n = n, mu = mu, sigma = sigma, nu = df)
> 
> ### check subset method
> model <- glmboostLSS(y ~ x1 + x2, families = StudentTLSS(),
+                      control = boost_control(mstop = 10),
+                      center = TRUE)
> model2 <- glmboostLSS(y ~ x1 + x2, families = StudentTLSS(),
+                           control = boost_control(mstop = 20),
+                           center = TRUE)
> model[20]

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = formula[[names(families)[[j]]]], data = data,     weights = w, center = TRUE, control = control, family = families[[j]])


	 Student's t-distribution: mu (id link) 

Loss function: { 
     -1 * (lgamma((df + 1)/2) - log(sigma) - lgamma(1/2) - lgamma(df/2) -  
         0.5 * log(df) - (df + 1)/2 * log(1 + (y - f)^2/(df *  
         sigma^2))) 
 } 
 

Number of boosting iterations: mstop = 20 
Step size:  0.1 
Offset:  0.5774545 

Coefficients: 
(Intercept)          x2 
  0.2309757  -0.4662895 
attr(,"offset")
[1] 0.5774545


	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = formula[[names(families)[[j]]]], data = data,     weights = w, center = TRUE, control = control, family = families[[j]])


	 Student's t-distribution: sigma (log link) 

Loss function: { 
     -1 * (lgamma((df + 1)/2) - f - lgamma(1/2) - lgamma(df/2) -  
         0.5 * log(df) - (df + 1)/2 * log(1 + (y - mu)^2/(df *  
         exp(2 * f)))) 
 } 
 

Number of boosting iterations: mstop = 20 
Step size:  0.1 
Offset:  1.152757 

Coefficients: 
(Intercept)          x1          x2 
 -0.9775043  -0.1715909   1.9935805 
attr(,"offset")
[1] 1.152757


	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = formula[[names(families)[[j]]]], data = data,     weights = w, center = TRUE, control = control, family = families[[j]])


	 Student's t-distribution: df (log link) 

Loss function: { 
     -1 * (lgamma((exp(f) + 1)/2) - log(sigma) - lgamma(1/2) -  
         lgamma(exp(f)/2) - 0.5 * f - (exp(f) + 1)/2 * log(1 +  
         (y - mu)^2/(exp(f) * sigma^2))) 
 } 
 

Number of boosting iterations: mstop = 20 
Step size:  0.1 
Offset:  0.8341285 

Coefficients: 
(Intercept)          x2 
  0.2077862  -0.1735146 
attr(,"offset")
[1] 0.8341285

> stopifnot(all.equal(coef(model),coef(model2)))
> stopifnot(length(coef(model2, aggregate = "none")[[1]][[1]]) ==
+           length(coef(model, aggregate = "none")[[1]][[1]]))
> stopifnot(length(coef(model2, aggregate = "none")[[1]][[1]]) == 20)
> 
> model <- glmboostLSS(y ~ x1 + x2, families = StudentTLSS(),
+                      control = boost_control(mstop = 10),
+                      center = TRUE)
> model2[10]

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = formula[[names(families)[[j]]]], data = data,     weights = w, center = TRUE, control = control, family = families[[j]])


	 Student's t-distribution: mu (id link) 

Loss function: { 
     -1 * (lgamma((df + 1)/2) - log(sigma) - lgamma(1/2) - lgamma(df/2) -  
         0.5 * log(df) - (df + 1)/2 * log(1 + (y - f)^2/(df *  
         sigma^2))) 
 } 
 

Number of boosting iterations: mstop = 10 
Step size:  0.1 
Offset:  0.5774545 

Coefficients: 
(Intercept)          x2 
 0.09857352 -0.19899842 
attr(,"offset")
[1] 0.5774545


	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = formula[[names(families)[[j]]]], data = data,     weights = w, center = TRUE, control = control, family = families[[j]])


	 Student's t-distribution: sigma (log link) 

Loss function: { 
     -1 * (lgamma((df + 1)/2) - f - lgamma(1/2) - lgamma(df/2) -  
         0.5 * log(df) - (df + 1)/2 * log(1 + (y - mu)^2/(df *  
         exp(2 * f)))) 
 } 
 

Number of boosting iterations: mstop = 10 
Step size:  0.1 
Offset:  1.152757 

Coefficients: 
(Intercept)          x2 
 -0.7647995   1.5439633 
attr(,"offset")
[1] 1.152757


	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = formula[[names(families)[[j]]]], data = data,     weights = w, center = TRUE, control = control, family = families[[j]])


	 Student's t-distribution: df (log link) 

Loss function: { 
     -1 * (lgamma((exp(f) + 1)/2) - log(sigma) - lgamma(1/2) -  
         lgamma(exp(f)/2) - 0.5 * f - (exp(f) + 1)/2 * log(1 +  
         (y - mu)^2/(exp(f) * sigma^2))) 
 } 
 

Number of boosting iterations: mstop = 10 
Step size:  0.1 
Offset:  0.8341285 

Coefficients: 
(Intercept)          x2 
  0.1252391  -0.1735146 
attr(,"offset")
[1] 0.8341285

> stopifnot(all.equal(coef(model),coef(model2)))
> stopifnot(length(coef(model2, aggregate = "none")[[1]][[1]]) ==
+           length(coef(model, aggregate = "none")[[1]][[1]]))
> stopifnot(length(coef(model2, aggregate = "none")[[1]][[1]]) == 10)
> 
> ### check trace
> model <- glmboostLSS(y ~ x1 + x2, families = StudentTLSS(),
+                      control = boost_control(mstop = 10, trace =TRUE),
+                      center = TRUE)
[  1] ........
Final risk: 13591.79 
> model[20]
[ 11] ........
Final risk: 13219.00 

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = formula[[names(families)[[j]]]], data = data,     weights = w, center = TRUE, control = control, family = families[[j]])


	 Student's t-distribution: mu (id link) 

Loss function: { 
     -1 * (lgamma((df + 1)/2) - log(sigma) - lgamma(1/2) - lgamma(df/2) -  
         0.5 * log(df) - (df + 1)/2 * log(1 + (y - f)^2/(df *  
         sigma^2))) 
 } 
 

Number of boosting iterations: mstop = 20 
Step size:  0.1 
Offset:  0.5774545 

Coefficients: 
(Intercept)          x2 
  0.2309757  -0.4662895 
attr(,"offset")
[1] 0.5774545


	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = formula[[names(families)[[j]]]], data = data,     weights = w, center = TRUE, control = control, family = families[[j]])


	 Student's t-distribution: sigma (log link) 

Loss function: { 
     -1 * (lgamma((df + 1)/2) - f - lgamma(1/2) - lgamma(df/2) -  
         0.5 * log(df) - (df + 1)/2 * log(1 + (y - mu)^2/(df *  
         exp(2 * f)))) 
 } 
 

Number of boosting iterations: mstop = 20 
Step size:  0.1 
Offset:  1.152757 

Coefficients: 
(Intercept)          x1          x2 
 -0.9775043  -0.1715909   1.9935805 
attr(,"offset")
[1] 1.152757


	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = formula[[names(families)[[j]]]], data = data,     weights = w, center = TRUE, control = control, family = families[[j]])


	 Student's t-distribution: df (log link) 

Loss function: { 
     -1 * (lgamma((exp(f) + 1)/2) - log(sigma) - lgamma(1/2) -  
         lgamma(exp(f)/2) - 0.5 * f - (exp(f) + 1)/2 * log(1 +  
         (y - mu)^2/(exp(f) * sigma^2))) 
 } 
 

Number of boosting iterations: mstop = 20 
Step size:  0.1 
Offset:  0.8341285 

Coefficients: 
(Intercept)          x2 
  0.2077862  -0.1735146 
attr(,"offset")
[1] 0.8341285

> 
> ### check formula-interface with lists
> set.seed(1907)
> n <- 5000
> x1  <- runif(n)
> x2 <- runif(n)
> mu <- 2 - 3*x2
> sigma <- exp(-1*x1 + 3*x2)
> df <- exp(1 + 3*x1)
> y <- rTF(n = n, mu = mu, sigma = sigma, nu = df)
> model <- glmboostLSS(list(mu = y ~ x2,
+                           sigma = y ~ x1 + x2,
+                           df = y ~ x1),
+                      families = StudentTLSS(),
+                      control = boost_control(mstop = 10, trace =TRUE),
+                      center = TRUE)
[  1] ........
Final risk: 13810.72 
> 
> stopifnot(all.equal(lapply(coef(model, which = ""), function(x) names(x)[-1]),
+                     list(mu = "x2", sigma = c("x1", "x2"), df = "x1")))
> 
> model <- glmboostLSS(list(mu = y ~ x2,
+                           df = y ~ x1,
+                           sigma = y ~ x1 + x2),
+                      families = StudentTLSS(),
+                      control = boost_control(mstop = 10, trace =TRUE),
+                      center = TRUE)
[  1] ........
Final risk: 13810.72 
> 
> stopifnot(all.equal(lapply(coef(model, which = ""), function(x) names(x)[-1]),
+                     list(mu = "x2", sigma = c("x1", "x2"), df = "x1")))
> 
> 
> ### check weights interface
> set.seed(1907)
> n <- 2500
> x1  <- runif(n)
> x2 <- runif(n)
> mu <- 2 - 3*x2
> sigma <- exp(-1*x1 + 3*x2)
> df <- exp(1 + 3*x1)
> y <- rTF(n = n, mu = mu, sigma = sigma, nu = df)
> dat <- data.frame(x1, x2, y)
> dat2 <- rbind(dat, dat) # data frame with duplicate entries
> 
> model <- glmboostLSS(list(mu = y ~ x2,
+                           sigma = y ~ x1 + x2,
+                           df = y ~ x1),
+                      data = dat, weights = rep(2, nrow(dat)),
+                      families = StudentTLSS(),
+                      control = boost_control(mstop = 10, trace =TRUE),
+                      center = TRUE)
[  1] ........
Final risk: 13810.99 
> 
> model2 <- glmboostLSS(list(mu = y ~ x2,
+                            sigma = y ~ x1 + x2,
+                            df = y ~ x1),
+                       data = dat2, families = StudentTLSS(),
+                       control = boost_control(mstop = 10, trace =TRUE),
+                       center = TRUE)
[  1] ........
Final risk: 13810.99 
> 
> stopifnot(all.equal(coef(model),coef(model2)))
> 
